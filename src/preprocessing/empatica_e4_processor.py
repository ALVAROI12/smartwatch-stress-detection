"""
Empatica E4 Data Processor
==========================

This module handles the preprocessing and feature extraction from Empatica E4 dataset.
Focused on smartwatch-compatible wrist sensors only.

Author: Alvaro Ibarra
Date: 2025
"""

import pandas as pd
import numpy as np
from pathlib import Path
import json
import warnings
from scipy import signal
from scipy.stats import skew, kurtosis
import matplotlib.pyplot as plt

warnings.filterwarnings('ignore')

class EmpaticaE4Processor:
    """
    Main class for processing Empatica E4 dataset (smartwatch-compatible sensors)
    """
    
    def __init__(self, data_path="data/empatica_e4_stress/Subjects", output_path="data/processed"):
        """
        Initialize the Empatica E4 processor
        
        Parameters:
        -----------
        data_path : str
            Path to Empatica E4 dataset directory (Subjects folder)
        output_path : str
            Path to save processed data
        """
        self.data_path = Path(data_path)
        self.output_path = Path(output_path)
        
        # Empatica E4 sampling rates
        self.sampling_rates = {
            'BVP': 64,    # PPG sensor
            'EDA': 4,     # Electrodermal activity
            'TEMP': 4,    # Temperature
            'ACC': 32,    # 3-axis accelerometer
            'HR': 1,      # Heart rate (derived, every 10s)
            'IBI': None   # Inter-beat intervals (irregular)
        }
        
        # Create output directory
        self.output_path.mkdir(parents=True, exist_ok=True)
        
        print(f"üîß Empatica E4 Processor initialized")
        print(f"   Data path: {self.data_path}")
        print(f"   Output path: {self.output_path}")
        print(f"   Expected sampling rates: {self.sampling_rates}")
    
    def load_subject_data(self, subject_dir):
        """
        Load all sensor data for a single subject
        
        Parameters:
        -----------
        subject_dir : Path
            Path to subject directory
            
        Returns:
        --------
        dict : Dictionary containing all sensor data
        """
        
        subject_id = subject_dir.name
        print(f"üìÇ Loading {subject_id}")
        
        data = {}
        
        # Load each sensor file
        for sensor in ['BVP', 'EDA', 'TEMP', 'ACC', 'HR', 'IBI']:
            file_path = subject_dir / f"{sensor}.csv"
            
            if file_path.exists():
                try:
                    df = pd.read_csv(file_path, header=None)
                    
                    # First row contains start timestamp
                    # Second row contains sampling rate
                    if len(df) >= 2:
                        start_time = df.iloc[0, 0]
                        sampling_rate = df.iloc[1, 0] if len(df.columns) == 1 else None
                        
                        # Extract actual data (skip first 2 rows)
                        sensor_data = df.iloc[2:].values
                        
                        data[sensor] = {
                            'start_time': start_time,
                            'sampling_rate': sampling_rate,
                            'data': sensor_data,
                            'length': len(sensor_data)
                        }
                        
                        print(f"   ‚úÖ {sensor}: {len(sensor_data)} samples at {sampling_rate}Hz")
                    else:
                        print(f"   ‚ö†Ô∏è  {sensor}: File too short")
                        
                except Exception as e:
                    print(f"   ‚ùå {sensor}: Error loading - {e}")
            else:
                print(f"   ‚ùå {sensor}: File not found")
        
        return data
    
    def create_labels_from_protocol(self, data_length, sampling_rate=4):
        """
        Create stress/baseline labels based on Empatica E4 protocol
        
        Note: This is a simplified version. In real implementation,
        you would need the actual protocol timing or manual annotation.
        
        Parameters:
        -----------
        data_length : int
            Length of the data
        sampling_rate : int
            Sampling rate for label generation
            
        Returns:
        --------
        np.array : Array of labels (0=baseline, 1=stress)
        """
        
        # Simplified protocol assumption:
        # First 25% = baseline
        # Middle 50% = stress
        # Last 25% = baseline
        
        labels = np.zeros(data_length)
        
        baseline1_end = int(0.25 * data_length)
        stress_start = baseline1_end
        stress_end = int(0.75 * data_length)
        
        # Mark stress period
        labels[stress_start:stress_end] = 1
        
        print(f"   üìä Labels created: {baseline1_end} baseline + {stress_end-stress_start} stress + {data_length-stress_end} baseline")
        
        return labels
    
    def synchronize_signals(self, data):
        """
        Synchronize all signals to common timebase (EDA sampling rate = 4Hz)
        
        Parameters:
        -----------
        data : dict
            Dictionary of sensor data
            
        Returns:
        --------
        dict : Synchronized sensor data
        """
        
        # Use EDA as reference (4Hz)
        reference_rate = 4
        reference_length = data['EDA']['length'] if 'EDA' in data else 1000
        
        synchronized = {}
        
        for sensor_name, sensor_info in data.items():
            if sensor_name in ['BVP', 'EDA', 'TEMP', 'ACC']:
                
                original_data = sensor_info['data']
                original_rate = sensor_info['sampling_rate']
                
                if original_rate and original_rate != reference_rate:
                    # Resample to reference rate
                    if sensor_name == 'ACC':
                        # 3D accelerometer data
                        resampled = np.zeros((reference_length, 3))
                        for axis in range(3):
                            if original_data.shape[1] > axis:
                                resampled[:, axis] = signal.resample(
                                    original_data[:, axis], reference_length
                                )
                        synchronized[sensor_name] = resampled
                    else:
                        # 1D sensor data
                        resampled = signal.resample(original_data.flatten(), reference_length)
                        synchronized[sensor_name] = resampled
                else:
                    # Already at reference rate or unknown rate
                    if sensor_name == 'ACC' and len(original_data.shape) > 1:
                        synchronized[sensor_name] = original_data[:reference_length, :]
                    else:
                        synchronized[sensor_name] = original_data.flatten()[:reference_length]
                
                print(f"   üîÑ {sensor_name}: Synchronized to {len(synchronized[sensor_name])} samples")
        
        return synchronized
    
    def extract_windows(self, synchronized_data, labels, window_size_sec=180, overlap_sec=90):
        """
        Extract windows from synchronized sensor data
        
        Parameters:
        -----------
        synchronized_data : dict
            Synchronized sensor data
        labels : np.array
            Labels array
        window_size_sec : int
            Window size in seconds
        overlap_sec : int
            Overlap in seconds
            
        Returns:
        --------
        list : List of windows with features
        """
        
        fs = 4  # Reference sampling rate
        window_size = window_size_sec * fs  # e.g., 180 * 4 = 720 samples
        overlap = overlap_sec * fs          # e.g., 90 * 4 = 360 samples
        step_size = window_size - overlap   # e.g., 360 samples
        
        print(f"   ü™ü Window extraction:")
        print(f"      Window size: {window_size_sec}s ({window_size} samples)")
        print(f"      Step size: {step_size} samples")
        
        windows = []
        data_length = len(labels)
        
        # Extract windows
        for start_idx in range(0, data_length - window_size + 1, step_size):
            end_idx = start_idx + window_size
            
            # Get window labels
            window_labels = labels[start_idx:end_idx]
            
            # Determine dominant label (majority vote)
            baseline_count = np.sum(window_labels == 0)
            stress_count = np.sum(window_labels == 1)
            
            # Require 80% purity
            total_count = len(window_labels)
            if baseline_count / total_count > 0.8:
                condition = 'baseline'
                label = 0
                purity = baseline_count / total_count
            elif stress_count / total_count > 0.8:
                condition = 'stress'
                label = 1
                purity = stress_count / total_count
            else:
                continue  # Skip mixed windows
            
            # Extract window data
            window_data = {}
            for sensor_name, sensor_data in synchronized_data.items():
                if sensor_name == 'ACC':
                    window_data[sensor_name] = sensor_data[start_idx:end_idx, :]
                else:
                    window_data[sensor_name] = sensor_data[start_idx:end_idx]
            
            # Extract features
            features = self.extract_window_features(window_data)
            
            if features is not None:
                features.update({
                    'condition': condition,
                    'label': label,
                    'window_start': start_idx,
                    'window_end': end_idx,
                    'purity': purity,
                    'window_duration_sec': window_size_sec
                })
                
                windows.append(features)
        
        return windows
    
    def extract_window_features(self, window_data):
        """
        Extract smartwatch-compatible features from window data
        
        Parameters:
        -----------
        window_data : dict
            Dictionary containing sensor data for one window
            
        Returns:
        --------
        dict : Extracted features
        """
        
        features = {}
        
        try:
            # 1. PPG/BVP Features (Universal smartwatch sensor)
            if 'BVP' in window_data:
                bvp = window_data['BVP']
                features.update({
                    'bvp_mean': float(np.mean(bvp)),
                    'bvp_std': float(np.std(bvp)),
                    'bvp_max': float(np.max(bvp)),
                    'bvp_min': float(np.min(bvp)),
                    'bvp_range': float(np.max(bvp) - np.min(bvp)),
                    'bvp_skew': float(skew(bvp)),
                    'bvp_kurt': float(kurtosis(bvp))
                })
                
                # Simple HRV features
                hrv_features = self.calculate_hrv_from_bvp(bvp)
                features.update(hrv_features)
            
            # 2. EDA Features (Premium smartwatch sensor)
            if 'EDA' in window_data:
                eda = window_data['EDA']
                features.update({
                    'eda_mean': float(np.mean(eda)),
                    'eda_std': float(np.std(eda)),
                    'eda_max': float(np.max(eda)),
                    'eda_min': float(np.min(eda)),
                    'eda_range': float(np.max(eda) - np.min(eda)),
                    'eda_skew': float(skew(eda)),
                    'eda_kurt': float(kurtosis(eda))
                })
                
                # EDA specific features
                eda_features = self.calculate_eda_features(eda)
                features.update(eda_features)
            
            # 3. Temperature Features (Common smartwatch sensor)
            if 'TEMP' in window_data:
                temp = window_data['TEMP']
                features.update({
                    'temp_mean': float(np.mean(temp)),
                    'temp_std': float(np.std(temp)),
                    'temp_max': float(np.max(temp)),
                    'temp_min': float(np.min(temp)),
                    'temp_range': float(np.max(temp) - np.min(temp)),
                    'temp_slope': float(self.calculate_slope(temp))
                })
            
            # 4. Accelerometer Features (Universal smartwatch sensor)
            if 'ACC' in window_data:
                acc = window_data['ACC']
                
                # Calculate magnitude
                acc_magnitude = np.sqrt(np.sum(acc**2, axis=1))
                
                features.update({
                    'acc_mean': float(np.mean(acc_magnitude)),
                    'acc_std': float(np.std(acc_magnitude)),
                    'acc_max': float(np.max(acc_magnitude)),
                    'acc_min': float(np.min(acc_magnitude)),
                    'acc_range': float(np.max(acc_magnitude) - np.min(acc_magnitude)),
                    'movement_intensity': float(np.mean(acc_magnitude)),
                    'movement_variability': float(np.std(acc_magnitude))
                })
                
                # Individual axis features
                for axis, axis_name in enumerate(['x', 'y', 'z']):
                    if acc.shape[1] > axis:
                        axis_data = acc[:, axis]
                        features.update({
                            f'acc_{axis_name}_mean': float(np.mean(axis_data)),
                            f'acc_{axis_name}_std': float(np.std(axis_data))
                        })
            
            return features
            
        except Exception as e:
            print(f"      ‚ùå Feature extraction error: {e}")
            return None
    
    def calculate_hrv_from_bvp(self, bvp_signal, fs=4):
        """Calculate simple HRV features from BVP signal"""
        
        try:
            # Simple peak detection
            peaks, _ = signal.find_peaks(bvp_signal, distance=int(fs*0.5))  # Min 0.5s between peaks
            
            if len(peaks) < 3:
                return {
                    'hr_mean': 70.0,
                    'hr_std': 5.0,
                    'rr_mean': 0.86,
                    'rr_std': 0.05
                }
            
            # Calculate RR intervals
            rr_intervals = np.diff(peaks) / fs
            
            # Remove outliers
            rr_clean = rr_intervals[(rr_intervals > 0.5) & (rr_intervals < 2.0)]
            
            if len(rr_clean) < 2:
                return {
                    'hr_mean': 70.0,
                    'hr_std': 5.0,
                    'rr_mean': 0.86,
                    'rr_std': 0.05
                }
            
            # Calculate features
            hr_values = 60 / rr_clean
            
            return {
                'hr_mean': float(np.mean(hr_values)),
                'hr_std': float(np.std(hr_values)),
                'rr_mean': float(np.mean(rr_clean)),
                'rr_std': float(np.std(rr_clean))
            }
            
        except Exception as e:
            return {
                'hr_mean': 70.0,
                'hr_std': 5.0,
                'rr_mean': 0.86,
                'rr_std': 0.05
            }
    
    def calculate_eda_features(self, eda_signal):
        """Calculate EDA-specific features"""
        
        try:
            # Peak detection for SCR (Skin Conductance Response)
            peaks, _ = signal.find_peaks(eda_signal, height=np.mean(eda_signal))
            
            # Tonic and phasic components (simplified)
            # Apply low-pass filter for tonic component
            sos = signal.butter(4, 0.05, btype='low', fs=4, output='sos')
            tonic = signal.sosfilt(sos, eda_signal)
            phasic = eda_signal - tonic
            
            return {
                'eda_peaks_count': float(len(peaks)),
                'eda_tonic_mean': float(np.mean(tonic)),
                'eda_phasic_std': float(np.std(phasic)),
                'eda_response_amplitude': float(np.std(phasic))
            }
            
        except Exception as e:
            return {
                'eda_peaks_count': 0.0,
                'eda_tonic_mean': 0.0,
                'eda_phasic_std': 0.0,
                'eda_response_amplitude': 0.0
            }
    
    def calculate_slope(self, signal_data):
        """Calculate trend slope of signal"""
        
        try:
            x = np.arange(len(signal_data))
            slope, _ = np.polyfit(x, signal_data, 1)
            return slope
        except:
            return 0.0
    
    def process_single_subject(self, subject_dir, window_size_sec=180, overlap_sec=90):
        """
        Process single Empatica E4 subject
        
        Parameters:
        -----------
        subject_dir : Path
            Path to subject directory
        window_size_sec : int
            Window size in seconds
        overlap_sec : int
            Overlap in seconds
            
        Returns:
        --------
        list : List of extracted windows with features
        """
        
        subject_id = subject_dir.name
        print(f"\nüîÑ Processing Empatica E4 Subject: {subject_id}")
        
        try:
            # 1. Load subject data
            raw_data = self.load_subject_data(subject_dir)
            
            if not raw_data:
                print(f"   ‚ùå No data loaded for {subject_id}")
                return []
            
            # 2. Synchronize signals
            synchronized_data = self.synchronize_signals(raw_data)
            
            # 3. Create labels (simplified - in real use, load from protocol file)
            data_length = len(synchronized_data.get('EDA', []))
            if data_length == 0:
                print(f"   ‚ùå No synchronized data for {subject_id}")
                return []
            
            labels = self.create_labels_from_protocol(data_length)
            
            # 4. Extract windows
            windows = self.extract_windows(synchronized_data, labels, window_size_sec, overlap_sec)
            
            # 5. Add subject ID to all windows
            for window in windows:
                window['subject_id'] = subject_id
            
            # Summary
            baseline_count = sum(1 for w in windows if w['condition'] == 'baseline')
            stress_count = sum(1 for w in windows if w['condition'] == 'stress')
            
            print(f"   üéØ {subject_id} Results:")
            print(f"      Total windows: {len(windows)}")
            print(f"      Baseline: {baseline_count}")
            print(f"      Stress: {stress_count}")
            
            return windows
            
        except Exception as e:
            print(f"   ‚ùå Error processing {subject_id}: {e}")
            return []
    
    def process_all_subjects(self, window_size_sec=180, overlap_sec=90):
        """
        Process all Empatica E4 subjects
        
        Parameters:
        -----------
        window_size_sec : int
            Window size in seconds
        overlap_sec : int
            Overlap in seconds
            
        Returns:
        --------
        list : All extracted windows from all subjects
        """
        
        print("üöÄ EMPATICA E4 PROCESSING - SMARTWATCH COMPATIBLE")
        print("=" * 55)
        print(f"üìÇ Data directory: {self.data_path}")
        print(f"‚öôÔ∏è  Window size: {window_size_sec} seconds")
        print(f"‚öôÔ∏è  Overlap: {overlap_sec} seconds")
        print(f"üéØ Focus: Wrist sensors only (smartwatch compatible)")
        
        all_windows = []
        processed_subjects = []
        
        # Find all subject directories
        subject_dirs = [d for d in self.data_path.iterdir() if d.is_dir() and d.name.startswith('subject_')]
        
        print(f"üìÇ Found {len(subject_dirs)} subject directories")
        
        for subject_dir in sorted(subject_dirs):
            try:
                windows = self.process_single_subject(subject_dir, window_size_sec, overlap_sec)
                if windows:
                    all_windows.extend(windows)
                    processed_subjects.append(subject_dir.name)
                
            except Exception as e:
                print(f"‚ùå Error processing {subject_dir.name}: {e}")
        
        # Save results
        if all_windows:
            output_file = self.output_path / "empatica_e4_features.json"
            
            with open(output_file, 'w') as f:
                json.dump(all_windows, f, indent=2)
            
            # Summary statistics
            baseline_windows = [w for w in all_windows if w['condition'] == 'baseline']
            stress_windows = [w for w in all_windows if w['condition'] == 'stress']
            
            print(f"\nüéØ EMPATICA E4 FINAL RESULTS")
            print("=" * 30)
            print(f"Subjects processed: {len(processed_subjects)}")
            print(f"Total windows: {len(all_windows)}")
            print(f"Baseline windows: {len(baseline_windows)}")
            print(f"Stress windows: {len(stress_windows)}")
            
            if len(all_windows) > 0:
                stress_percentage = (len(stress_windows) / len(all_windows)) * 100
                baseline_percentage = (len(baseline_windows) / len(all_windows)) * 100
                print(f"Class balance: {baseline_percentage:.1f}% baseline, {stress_percentage:.1f}% stress")
            
            print(f"Results saved to: {output_file}")
            
            if len(stress_windows) > 0:
                print(f"\nüéâ SUCCESS! Smartwatch-compatible features extracted!")
                print(f"üì± Ready for TicWatch3 and other smartwatch deployment!")
            else:
                print(f"\n‚ö†Ô∏è  No stress windows found - check label generation")
            
            return all_windows
        else:
            print(f"\n‚ùå No windows extracted!")
            return []

# Main execution function
def main():
    """Main function to run Empatica E4 processing"""
    
    # Initialize processor
    processor = EmpaticaE4Processor(
        data_path="data/empatica_e4_stress/Subjects",
        output_path="data/processed"
    )
    
    # Process all subjects
    windows = processor.process_all_subjects(
        window_size_sec=180,  # 3 minutes
        overlap_sec=90        # 50% overlap
    )
    
    return windows

if __name__ == "__main__":
    windows = main()